# headfirstpytorch
This is inspired after a key book in Java which i have during my growing years in Tech, it is very good it starts with an usecase and then explains the details when the situation arises, on the tutorials i have found online there is no such feeling of adrenaline rush , this is an attempt to create such content, hope you like it.

If i interview a person on DL interview , i hope the person is good in basics of ML and DL, there are lot of theories on github repos but i will try to cover what is not covered in them.

most of the activitiy here will be done using pytorch. which will start with a problem statement and just reserach before going deep into the implemenation.


**challenge 1**:
can you classify text using Multilevel perceptron.i will give you the amazon review dataset tell me how you will approach it.

**challenge 2**:
can you take any other model and improve the performance in challenge 1, what is the response time.

**challenge 3**:
What is FlashAttention and how does it work?
can you write code for that?

**challenge 4**:
What is KV cache and why is it useful?

**challenge 5**:
Why is LLM inference memory-bounded?

**challenge 6**:
What are scaling laws for LLMs? what is the use for you regarding that?can you show me a demonstration.

**challenge 7**:
What is LoRA and how does it work?

**challenge 8**:
What is the difference between the Transformer and RNNs?

**challenge 9**:
Difference between LSTM and vanilla RNN.can you demo the code and explain the difference.

**challenge 10**
Difference between structured prediction and classification.

**challenge 11**
Difference between CRFs and HMMs.

**challenge 12**
What is the difference between a LM and a LLM?

**challenge 13**
Instruction tuning, in-context learning, RLHF, etc.

**challenge 14**
Pitfalls of n-gram-based metrics like ROUGE or BLEU.

**challenge 15**
Differences between encoder-only models, encoder-decoder models, and decoder-only models. Examples as well.

**challenge 16**:
Why do so many models seem to be decoder-only these days?

**challenge 17**:
Should be able to code basic transformers from scratch. Implement KV caching. Understand different positional encodings techniques.

**challenge 18**:
can you tell about below topics:
Evaluation
Fine-tuning techniques
RAGs
NLP fundamentals

Understanding how LLMs work (internals)

what are the evaluations
interview:(https://www.youtube.com/watch?v=sGqEKzJYrNE)

ROUGE, BLEU might work
https://blog.vespa.ai/improving-retrieval-with-llm-as-a-judge/

challenge 19:
quantization techniques AWQ GPTQ papers have you read them?

challenge 20:
hparameter tuning and parallelization techniques?

challenge 21:
go deep on finetuning what are the tradeoffs why you chose one over the other

challenges 22:
can you explain RAG and its various components?

